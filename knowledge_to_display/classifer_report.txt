
•Random Forest Classifier function



Importing the essential tools
from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import RandomForestClassifier

Function definition
def randomforestclassifier(X_train,X_test,y_train,y_test):

    classifier = RandomForestClassifier()
    clffit = classifier.fit(X_train,y_train)
    parameters = [{'max_depth':[None]}]
    gs = GridSearchCV(estimator = clffit,
                      param_grid = parameters,
                      n_jobs = -1,
                      scoring = 'accuracy',
                      cv = 2)
    gs.fit(X_train, y_train)
    classifier = gs.best_estimator_
    classifier.fit(X_train,y_train)
    return classifier.predict(X_test), gs.best_params_


Function call
randomforestclassifier(X_train,X_test,y_train,y_test)

Learn more here : 
Check out for more info : 


•Decision Tree Classifier function





Importing the essential tools
from sklearn.model_selection import GridSearchCV
from sklearn.tree import DecisionTreeClassifier

Function definition
def decisiontreeclassifier(X_train,X_test,y_train,y_test):
    classifier = DecisionTreeClassifier()
    clffit = classifier.fit(X_train,y_train)
    parameters = [{'splitter':['best']}]
    gs = GridSearchCV(estimator = clffit,
                      param_grid = parameters,
                      n_jobs = -1,
                      scoring = 'accuracy',
                      cv = 2)
    gs.fit(X_train, y_train)
    classifier = gs.best_estimator_
    classifier.fit(X_train,y_train)
    return classifier.predict(X_test), gs.best_params_


Function call
decisiontreeclassifier(X_train,X_test,y_train,y_test)

For more info visit : https://en.wikipedia.org/wiki/Tikhonov_regularization
Learn more : https://youtu.be/Q81RR3yKn30






•Lasso Regression function
Lasso regression analysis is a shrinkage and variable selection method for linear regression models.
The goal of lasso regression is to obtain the subset of predictors that minimizes prediction error for a quantitative response variable.

Importing the essential tools
from sklearn.model_selection import GridSearchCV
from sklearn.linear_model import Lasso

Function definition
def lassoregressor(X_train,X_test,y_train,y_test):
    regressor=Lasso()
    parameters=[{'random_state':[None]}]
    regressor=GridSearchCV(regressor,parameters,scoring='r2',cv=2)
    regressor.fit(X_train,y_train)
    return regressor.predict(X_test), regressor.best_params_, regressor.score(X_test,y_test)

Function call
lassoregressor(X_train,X_test,y_train,y_test)

Read more here : https://www.analyticsvidhya.com/blog/2016/01/ridge-lasso-regression-python-complete-tutorial/
For more info visit : https://youtu.be/9lRv01HDU0s
Check out more : https://youtu.be/NGf0voTMlcs






•Decision Tree regression function
Decision Tree algorithm first sorts out all the values and keeps a threshold value, based on which it creates branching. This is continued until we arrive at the leaf nodes.
Decision tree regressor builds regression models in the form of a tree structure.
It breaks down a dataset into smaller and smaller subsets while at the same time an associated decision tree is incrementally developed.
The final result is a tree with decision nodes and leaf nodes.

Importing the essential tools
from sklearn.model_selection import GridSearchCV
from sklearn.tree import DecisionTreeRegressor

Function definition
def decisiontreeregressor(X_train,X_test,y_train,y_test):
    regressor = DecisionTreeRegressor()
    parameters=[{'max_depth':[None]}]
    regressor=GridSearchCV(regressor,parameters,scoring='r2',cv=2)
    regressor.fit(X,y)
    return regressor.predict(X_test), regressor.best_params_, regressor.score(X_test,y_test)

Function call
decisiontreeregressor(X_train,X_test,y_train,y_test)

Check out the documentation : https://scikit-learn.org/stable/auto_examples/tree/plot_tree_regression.html
Learn more : https://en.wikipedia.org/wiki/Decision_tree_learning
For more info check : https://youtu.be/g9c66TUylZ4




•Gradient Boosting regression function
Gradient boosting is one of the most powerful techniques for building predictive models.
Boosting is a method of converting weak learners into strong learners.
It becomes very handy in Kaggle competitions and is heavily used in such scenarios.


Importing the essential tools
from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import GradientBoostingRegressor

Function definition
def gradientboostingregressor(X_train,X_test,y_train,y_test):
        regressor = GradientBoostingRegressor()
        parameters ={'max_features':[None]}
        regressor = GridSearchCV(regressor,parameters,scoring='r2', cv=2)
        regressor.fit(X_train,y_train)
        return regressor.predict(X_test), regressor.best_params_, regressor.score(X_test,y_test)

Function call
gradientboostingregressor(X_train,X_test,y_train,y_test)

Check out the documentation here : https://scikit-learn.org/stable/auto_examples/ensemble/plot_gradient_boosting_regression.html
Learn more : https://en.wikipedia.org/wiki/Gradient_boosting